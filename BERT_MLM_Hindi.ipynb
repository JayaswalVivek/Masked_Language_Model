{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_MLM_Hindi.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNb7I4FLnpC+OzDIHErtYxS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3k3pb_8BN70h"},"source":["## **Creation of a masked language model for Hindi**\n","\n","Date: 01-Dec-2021 \\\n","Author: Vivek Jayaswal"]},{"cell_type":"code","metadata":{"id":"11C-iG_NQpYC"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiG-3y7mIu0G"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lz-PbTa9N2xY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXKWVBQpQMYc"},"source":["# Change the present working directory to the parent directory for WikiText\n","import os\n","os.chdir('/content/drive/MyDrive/Data/')\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i44nmARbIO0O"},"source":["### **Section 1: Generate byte-level BPE tokens for a corpus**\n","**Section 1.1: Train the tokenizer**"]},{"cell_type":"code","metadata":{"id":"1A7jVYz5RCNl"},"source":["import tokenizers\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","\n","tokenizer = Tokenizer(BPE(unk_token=\"<unk>\")) #BytePair Encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aD1h2HK7RP7T"},"source":["tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.BertPreTokenizer()\n","# tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel() # Byte-level BPE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-AmRxQ7rbGE"},"source":["tokenizer.enable_truncation(512)\n","tokenizer.normalizer = tokenizers.normalizers.BertNormalizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SefM_Lr7RNwt"},"source":["from tokenizers.trainers import BpeTrainer\n","# trainer = BpeTrainer(special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n","trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aF5f1wYRYaz"},"source":["files = [\"./Hindi_Input/Mstr_Hindi_Rev.txt\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMFZM2ScRkPz"},"source":["# Execution time: 55s (for Hindi data)\n","tokenizer.train(files, trainer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wkj2ajN8RkU4"},"source":["# tokenizer.save(\"./Hindi_Output/tokenizer.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUUrRW1CRmnW"},"source":["**Section 1.2: Load and test the tokenizer**\n"]},{"cell_type":"code","metadata":{"id":"vzyQBE7QSOJr"},"source":["tokenizer_load = Tokenizer.from_file(\"./Hindi_Output/tokenizer.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--gn7AEqSxLZ"},"source":["# Use of RoBERTa's special tokens at beginning and end of sentence\n","tokenizer_load.post_processor = tokenizers.processors.BertProcessing(sep=(\"[SEP]\", tokenizer_load.token_to_id(\"[SEP]\"))\n","                                                                  , cls=(\"[CLS]\", tokenizer_load.token_to_id(\"[CLS]\")))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gxe4zPcdUVz7"},"source":["output = tokenizer_load.encode(\"एस एक बांग्ला टीवी चैनल है\")\n","print(output.tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DdFVVF_oUeYy"},"source":["print(output.type_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L75QE_2IIZc_"},"source":["### **Section 2: Train a masked LM using the tokenizer trained & saved in Section 1**"]},{"cell_type":"code","metadata":{"id":"Pksgg-naKBi_"},"source":["from transformers import BertConfig\n","\n","config = BertConfig(\n","    vocab_size=30000,  # value of 30K was chosen as the tokenizer was trained with a default value of 30K\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=2,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCkD9oPjQsGG"},"source":["# Save config.json\n","# config.to_json_file('./Hindi_Output/config.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndXgvTwOKSAQ"},"source":["# from transformers import RobertaTokenizerFast\n","from transformers import BertTokenizerFast"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jub0bwP0KSHQ"},"source":["tokenizer_new = BertTokenizerFast.from_pretrained(\"./Hindi_Output\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cJGCrSJMT-r"},"source":["from transformers import BertForMaskedLM\n","model = BertForMaskedLM(config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mmVrrqFMY96"},"source":["from transformers import DataCollatorForLanguageModeling\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer_new, mlm=True, mlm_probability=0.15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39yLkrFGUw_e"},"source":["# !mkdir ./shards\n","!split -a 40 -l 12600 -d \"./Hindi_Input/Mstr_Hindi_Rev.txt\" ./shards_hindi/shard_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZs5-eZZUxlw"},"source":["import glob\n","files = glob.glob('./shards_hindi/*')\n","# files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Col73VQOVS18"},"source":["from datasets import load_dataset\n","dataset = load_dataset('text', data_files=files[0], split='train') #Use only one batch of 256000 examples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLEpeRtc9w6w"},"source":["print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEIvajHgUxrp"},"source":["def encode(examples):\n","  return tokenizer_new(examples['text'], truncation=True, padding='max_length', max_length=512)\n","\n","dataset = dataset.map(encode, batched=True) # Apply the \"encode\" function to all elements of \"dataset\" which is passed as \"example\" variable\n","dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5BzC3FHMeln"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./Hindi_Output\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=8, # lowered the batch size from 64\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R6czungT4Pf"},"source":["%%time\n","# trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"knICLNJpSwwA"},"source":["# The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text.\n","# ***** Running training *****\n","#   Num examples = 12601\n","#   Num Epochs = 1\n","#   Instantaneous batch size per device = 8\n","#   Total train batch size (w. parallel, distributed & accumulation) = 8\n","#   Gradient Accumulation steps = 1\n","#   Total optimization steps = 1576\n","\n","# [1576/1576 28:37, Epoch 1/1]\n","# Step \tTraining Loss\n","# 500 \t7.680700\n","# 1000 \t7.104600\n","# 1500 \t6.969000\n","\n","# Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","# CPU times: user 28min 31s, sys: 5.58 s, total: 28min 36s\n","# Wall time: 28min 39s\n","\n","# TrainOutput(global_step=1576, training_loss=7.2410494421944405, metrics={'train_runtime': 1719.0714, 'train_samples_per_second': 7.33, 'train_steps_per_second': 0.917, 'total_flos': 1670382921203712.0, 'train_loss': 7.2410494421944405, 'epoch': 1.0})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pzz2TCqfTuhS"},"source":["# trainer.save_model(\"./Hindi_Model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-uPSV0ziLLqe"},"source":["### **Section 3: Load and test the trained masked LM**"]},{"cell_type":"code","metadata":{"id":"yP2tBjjPHZG1"},"source":["from transformers import pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uR1RSrYLaE5"},"source":["fill_mask = pipeline(\"fill-mask\", model=\"./Hindi_Model\", tokenizer=\"./Hindi_Model\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HknWgXoTLjff"},"source":["# Poor fit owing to the limited size of the training set\n","\n","fill_mask(\"एस एक बांग्ला [MASK] चैनल है\")\n","\n","# [{'score': 0.049333274364471436,\n","#   'sequence': 'एस एक बागला । चनल ह',\n","#   'token': 375,\n","#   'token_str': '।'},\n","#  {'score': 0.046125490218400955,\n","#   'sequence': 'एस एक बागला ह चनल ह',\n","#   'token': 363,\n","#   'token_str': 'ह'},\n","#  {'score': 0.03156042471528053,\n","#   'sequence': 'एस एक बागला म चनल ह',\n","#   'token': 354,\n","#   'token_str': 'म'},\n","#  {'score': 0.03022793121635914,\n","#   'sequence': 'एस एक बागला क चनल ह',\n","#   'token': 330,\n","#   'token_str': 'क'},\n","#  {'score': 0.026870885863900185,\n","#   'sequence': 'एस एक बागला, चनल ह',\n","#   'token': 16,\n","#   'token_str': ','}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPITJZamT37n"},"source":["# fill_mask(\"भारत में प्रचलित कुछ अन्य प्राचीन संवत इस प्रकार है\")\n","fill_mask(\"भारत में प्रचलित [MASK] अन्य संवत इस प्रकार है\")\n","\n","# [{'score': 0.04326169565320015,\n","#   'sequence': 'भारत म परचलित ह अनय सवत इस परकार ह',\n","#   'token': 363,\n","#   'token_str': 'ह'},\n","#  {'score': 0.04239127039909363,\n","#   'sequence': 'भारत म परचलित म अनय सवत इस परकार ह',\n","#   'token': 354,\n","#   'token_str': 'म'},\n","#  {'score': 0.04220021516084671,\n","#   'sequence': 'भारत म परचलित । अनय सवत इस परकार ह',\n","#   'token': 375,\n","#   'token_str': '।'},\n","#  {'score': 0.038384582847356796,\n","#   'sequence': 'भारत म परचलित क अनय सवत इस परकार ह',\n","#   'token': 330,\n","#   'token_str': 'क'},\n","#  {'score': 0.02418365515768528,\n","#   'sequence': 'भारत म परचलित - अनय सवत इस परकार ह',\n","#   'token': 17,\n","#   'token_str': '-'}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVtz9RumUXJl"},"source":[""],"execution_count":null,"outputs":[]}]}